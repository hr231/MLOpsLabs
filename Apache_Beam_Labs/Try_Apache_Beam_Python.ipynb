{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNKIMlEDZ_Vw"
   },
   "source": [
    "# Apache Beam with NLP Enhancement - Named Entity Recognition\n",
    "\n",
    "This notebook demonstrates how to use Apache Beam for data processing with an enhanced focus on **Named Entity Recognition (NER)** using spaCy. Instead of simple word counting, we'll extract meaningful entities like character names, locations, and organizations from Shakespeare's King Lear.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand Apache Beam's programming model and pipeline structure\n",
    "- Learn how to integrate NLP libraries (spaCy) with Apache Beam\n",
    "- Extract and analyze named entities from text data\n",
    "- Compare traditional word counting with entity-based analysis\n",
    "\n",
    "## What one will Learn of it\n",
    "\n",
    "1. **Basic Apache Beam Concepts**: PCollections, Transforms, and Pipelines\n",
    "2. **NLP Integration**: Using spaCy for Named Entity Recognition\n",
    "3. **Entity Analysis**: Extracting PERSON, GPE (Geopolitical), and ORG entities\n",
    "4. **Data Processing**: Counting entity frequencies and generating insights\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To navigate through different sections, use the table of contents. From **View** drop-down list, select **Table of contents**.\n",
    "\n",
    "To run a code cell, you can click the **Run cell** button at the top left of the cell, or select it and press **`Shift+Enter`**. Try modifying a code cell and re-running it to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "GOOk81Jj_yUy",
    "outputId": "d283dfb2-4f51-4fec-816b-f57b0cb9b71c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Apache Beam and spaCy...\n",
      ">> pip install --quiet apache-beam spacy\n",
      "\n",
      "Downloading English language model...\n",
      ">> python -m spacy download en_core_web_sm\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 3.1/12.8 MB 29.0 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 27.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 25.3 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "Setting up data directory...\n",
      ">> mkdir -p data\n",
      "\n",
      ">> gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data already exists.\n",
      "Error occurred while processing: data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setup complete! Ready to process text with NLP.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gsutil' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Run and print a shell command.\n",
    "def run(cmd):\n",
    "  print('>> {}'.format(cmd))\n",
    "  !{cmd}\n",
    "  print('')\n",
    "\n",
    "# Install required packages\n",
    "print(\"Installing Apache Beam and spaCy...\")\n",
    "run('pip install --quiet apache-beam spacy')\n",
    "\n",
    "# Download the English language model for spaCy\n",
    "print(\"Downloading English language model...\")\n",
    "run('python -m spacy download en_core_web_sm')\n",
    "\n",
    "# Create data directory and download sample text\n",
    "print(\"Setting up data directory...\")\n",
    "run('mkdir -p data')\n",
    "\n",
    "# Download King Lear text file (if not already present)\n",
    "import os\n",
    "if not os.path.exists('data/kinglear.txt'):\n",
    "    print(\"Downloading King Lear text file...\")\n",
    "    run('gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/')\n",
    "else:\n",
    "    print(\"King Lear text file already exists.\")\n",
    "\n",
    "print(\"Setup complete! Ready to process text with NLP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Enhancement - Named Entity Recognition\n",
    "\n",
    "\n",
    "- **PERSON**: Character names (e.g., \"King Lear\", \"Cordelia\", \"Edmund\")\n",
    "- **GPE**: Geopolitical entities like countries and cities (e.g., \"Britain\", \"France\")\n",
    "- **ORG**: Organizations and institutions (e.g., \"Duke of Cornwall\")\n",
    "\n",
    "### Why Named Entity Recognition?\n",
    "\n",
    "Instead of counting every word, NER helps us:\n",
    "1. **Focus on meaningful content**: Extract only relevant entities\n",
    "2. **Reduce noise**: Filter out common words like \"the\", \"and\", \"of\"\n",
    "3. **Gain insights**: Understand character relationships and locations\n",
    "4. **Improve analysis**: Get structured data from unstructured text\n",
    "\n",
    "### spaCy Integration\n",
    "\n",
    "SpaCy's pre-trained English model (`en_core_web_sm`) which provides:\n",
    "- High accuracy entity recognition\n",
    "- Multiple entity types\n",
    "- Fast processing suitable for Apache Beam pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import GCSFileSystem; loading of this filesystem will be skipped. Error details: cannot import name 'storage' from 'google.cloud' (unknown location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy English model...\n",
      "Sample text: King Lear of Britain had three daughters: Goneril, Regan, and Cordelia.\n",
      "Extracted entities:\n",
      "  - Britain (GPE)\n",
      "  - Goneril (GPE)\n",
      "  - Regan (GPE)\n",
      "  - Cordelia (GPE)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import apache_beam as beam\n",
    "import spacy\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Load the English language model\n",
    "print(\"Loading spaCy English model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract named entities from text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to process\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: List of (entity_text, entity_type) tuples\n",
    "    \"\"\"\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities of interest\n",
    "    entities = []\n",
    "    target_types = {'PERSON', 'GPE', 'ORG'}  # Person, Geopolitical, Organization\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in target_types:\n",
    "            # Clean and normalize entity text\n",
    "            entity_text = ent.text.strip()\n",
    "            if len(entity_text) > 1:  # Filter out single characters\n",
    "                entities.append((entity_text, ent.label_))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test the entity extraction function\n",
    "sample_text = \"King Lear of Britain had three daughters: Goneril, Regan, and Cordelia.\"\n",
    "print(f\"Sample text: {sample_text}\")\n",
    "print(\"Extracted entities:\")\n",
    "for entity, entity_type in extract_entities(sample_text):\n",
    "    print(f\"  - {entity} ({entity_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-HubCrk-h_G"
   },
   "source": [
    "## Enhanced Entity Count Pipeline\n",
    "\n",
    "This pipeline will:\n",
    "\n",
    "1. **Read** the King Lear text file\n",
    "2. **Extract** named entities using spaCy\n",
    "3. **Count** entity frequencies by type\n",
    "4. **Output** structured results showing entity counts\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "The pipeline follows Apache Beam's standard pattern:\n",
    "- **Source**: Read text files\n",
    "- **Transform**: Extract entities and count frequencies  \n",
    "- **Sink**: Write results to output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1173
    },
    "id": "x_D7sxUHFzUp",
    "outputId": "44c926df-aa4a-4bea-9247-27c7cb537717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced Entity Count Pipeline...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline execution completed!\n",
      "==================================================\n",
      "Entity Count Results:\n",
      "------------------------------\n",
      ">> head -n 50 outputs/entity_counts-00000-of-*\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Entity Count Pipeline with NLP\n",
    "inputs_pattern = 'data/kinglear.txt'\n",
    "outputs_prefix = 'outputs/entity_counts'\n",
    "\n",
    "print(\"Starting Enhanced Entity Count Pipeline...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Running locally in the DirectRunner with entity extraction\n",
    "# Note: For large files, consider processing in smaller chunks to avoid timeout issues\n",
    "with beam.Pipeline() as pipeline:\n",
    "    # Store the entity counts in a PCollection.\n",
    "    # Each element is a tuple of ((entity, entity_type), count)\n",
    "    entity_counts = (\n",
    "        # Start with the pipeline\n",
    "        pipeline\n",
    "\n",
    "        # Read lines from the text file\n",
    "        | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
    "        # Element type: str - text line\n",
    "\n",
    "        # Extract named entities using our spaCy function\n",
    "        | 'Extract entities' >> beam.FlatMap(extract_entities)\n",
    "        # Element type: (str, str) - (entity_text, entity_type)\n",
    "\n",
    "        # Create key-value pairs for counting\n",
    "        | 'Pair entities with 1' >> beam.Map(lambda entity: (entity, 1))\n",
    "        # Element type: ((str, str), int) - ((entity, type), 1)\n",
    "\n",
    "        # Group by entity and sum the counts\n",
    "        | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "        # Element type: ((str, str), int) - ((entity, type), count)\n",
    "    )\n",
    "\n",
    "    # Process the entity counts for output\n",
    "    (\n",
    "        entity_counts\n",
    "\n",
    "        # Format results with entity type information\n",
    "        | 'Format results' >> beam.Map(\n",
    "            lambda entity_count: f\"{entity_count[0][0]} ({entity_count[0][1]}): {entity_count[1]}\"\n",
    "        )\n",
    "        # Element type: str - formatted result line\n",
    "\n",
    "        # Write results to file\n",
    "        | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
    "    )\n",
    "\n",
    "print(\"Pipeline execution completed!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display the results\n",
    "print(\"Entity Count Results:\")\n",
    "print(\"-\" * 30)\n",
    "run('head -n 50 {}-00000-of-*'.format(outputs_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "\n",
    "\n",
    "1. **Processed** Shakespeare's King Lear using Apache Beam\n",
    "2. **Extracted** named entities using spaCy's NLP capabilities\n",
    "3. **Counted** entity frequencies by type (PERSON, GPE, ORG)\n",
    "4. **Generated** structured insights from unstructured text\n",
    "\n",
    "### Key Benefits of NER vs. Word Counting\n",
    "\n",
    "| Aspect | Traditional Word Count | NER Entity Count |\n",
    "|--------|----------------------|------------------|\n",
    "| **Focus** | All words (including \"the\", \"and\", \"of\") | Only meaningful entities |\n",
    "| **Insights** | Basic frequency analysis | Character and location analysis |\n",
    "| **Noise** | High (common words dominate) | Low (filtered to relevant content) |\n",
    "| **Structure** | Unstructured word list | Categorized by entity type |\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "From King Lear, you should see entities like:\n",
    "- **PERSON**: King Lear, Cordelia, Edmund, Gloucester, Kent\n",
    "- **GPE**: Britain, France, Cornwall, Albany\n",
    "- **ORG**: Duke of Cornwall, Earl of Kent\n",
    "\n",
    "This provides much more meaningful insights than counting every occurrence of \"the\" or \"and\"!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Entity Types...\n",
      "========================================\n",
      "\n",
      "PERSON Entities (Top 10):\n",
      "------------------------------\n",
      "  REGAN: 69\n",
      "  Tom: 21\n",
      "  Kent: 16\n",
      "  KENT: 9\n",
      "  Shall: 7\n",
      "  Edgar: 7\n",
      "  ho: 7\n",
      "  Gloucester: 7\n",
      "  Burgundy: 6\n",
      "  Gentleman: 5\n",
      "\n",
      "GPE Entities (Top 10):\n",
      "------------------------------\n",
      "  thou: 32\n",
      "  Thou: 29\n",
      "  France: 21\n",
      "  Cordelia: 20\n",
      "  Dover: 15\n",
      "  Gloucester: 14\n",
      "  Regan: 13\n",
      "  FRANCE: 8\n",
      "  Albany: 8\n",
      "  Goneril: 7\n",
      "\n",
      "ORG Entities (Top 10):\n",
      "------------------------------\n",
      "  ALBANY: 60\n",
      "  Gentleman: 22\n",
      "  GLOUCESTER: 18\n",
      "  EDGAR: 16\n",
      "  Gloucester: 15\n",
      "  KENT: 12\n",
      "  GONERIL: 5\n",
      "  Burgundy: 5\n",
      "  Nay: 3\n",
      "  EDGAR]: 3\n",
      "\n",
      "Summary:\n",
      "  Total PERSON entities: 103\n",
      "  Total GPE entities: 55\n",
      "  Total ORG entities: 130\n",
      "\n",
      "Enhanced Apache Beam NLP Pipeline Complete!\n",
      "You've successfully integrated spaCy with Apache Beam for entity extraction!\n"
     ]
    }
   ],
   "source": [
    "# Additional Analysis: Entity Type Breakdown\n",
    "print(\"Analyzing Entity Types...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Read and analyze the results\n",
    "try:\n",
    "    with open('outputs/entity_counts-00000-of-00001', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Parse results and group by entity type\n",
    "    entity_types = {'PERSON': [], 'GPE': [], 'ORG': []}\n",
    "    \n",
    "    for line in lines:\n",
    "        if '(' in line and ')' in line:\n",
    "            # Parse format: \"Entity (TYPE): count\"\n",
    "            parts = line.strip().split(' (')\n",
    "            if len(parts) == 2:\n",
    "                entity = parts[0]\n",
    "                type_and_count = parts[1].split('): ')\n",
    "                if len(type_and_count) == 2:\n",
    "                    entity_type = type_and_count[0]\n",
    "                    count = int(type_and_count[1])\n",
    "                    \n",
    "                    if entity_type in entity_types:\n",
    "                        entity_types[entity_type].append((entity, count))\n",
    "    \n",
    "    # Display results by type\n",
    "    for entity_type, entities in entity_types.items():\n",
    "        if entities:\n",
    "            print(f\"\\n{entity_type} Entities (Top 10):\")\n",
    "            print(\"-\" * 30)\n",
    "            # Sort by count and show top 10\n",
    "            sorted_entities = sorted(entities, key=lambda x: x[1], reverse=True)[:10]\n",
    "            for entity, count in sorted_entities:\n",
    "                print(f\"  {entity}: {count}\")\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total PERSON entities: {len(entity_types['PERSON'])}\")\n",
    "    print(f\"  Total GPE entities: {len(entity_types['GPE'])}\")\n",
    "    print(f\"  Total ORG entities: {len(entity_types['ORG'])}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Output file not found. Please run the pipeline first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing results: {e}\")\n",
    "\n",
    "print(\"\\nEnhanced Apache Beam NLP Pipeline Complete!\")\n",
    "print(\"You've successfully integrated spaCy with Apache Beam for entity extraction!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Small Sample Processing\n",
    "\n",
    "If you encounter timeout issues with the full file, here's a version that processes just the first few lines to demonstrate the concept:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing small sample (first 50 lines)...\n",
      "Loaded 50 lines for processing\n",
      "Sample Entity Count Results:\n",
      "------------------------------\n",
      "  Britain (GPE): 1\n",
      "  FRANCE (GPE): 1\n",
      "  DUKE OF ALBANY (ORG): 1\n",
      "  ALBANY (ORG): 1\n",
      "  EDGAR (ORG): 1\n",
      "  Gloucester (ORG): 3\n",
      "  Goneril (GPE): 1\n",
      "  Gentleman (ORG): 2\n",
      "  Cordelia (GPE): 1\n",
      "  Herald (ORG): 1\n",
      "  Cornwall (GPE): 1\n",
      "Sample processing completed!\n"
     ]
    }
   ],
   "source": [
    "# Small Sample Processing - Process first 50 lines only\n",
    "print(\"Processing small sample (first 50 lines)...\")\n",
    "\n",
    "# Read first 50 lines from the file\n",
    "sample_lines = []\n",
    "with open('data/kinglear.txt', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 50:  # Only process first 50 lines\n",
    "            break\n",
    "        sample_lines.append(line.strip())\n",
    "\n",
    "print(f\"Loaded {len(sample_lines)} lines for processing\")\n",
    "\n",
    "# Process the sample with Apache Beam\n",
    "with beam.Pipeline() as pipeline:\n",
    "    entity_counts = (\n",
    "        pipeline\n",
    "        | 'Create sample data' >> beam.Create(sample_lines)\n",
    "        | 'Extract entities' >> beam.FlatMap(extract_entities)\n",
    "        | 'Pair entities with 1' >> beam.Map(lambda entity: (entity, 1))\n",
    "        | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = (\n",
    "        entity_counts\n",
    "        | 'Format results' >> beam.Map(\n",
    "            lambda entity_count: f\"{entity_count[0][0]} ({entity_count[0][1]}): {entity_count[1]}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Collect and display results\n",
    "    results = formatted_results | beam.combiners.ToList()\n",
    "    \n",
    "    def print_sample_results(results):\n",
    "        print(\"Sample Entity Count Results:\")\n",
    "        print(\"-\" * 30)\n",
    "        for result in results:\n",
    "            print(f\"  {result}\")\n",
    "    \n",
    "    results | beam.Map(print_sample_results)\n",
    "\n",
    "print(\"Sample processing completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Try Apache Beam - Python",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
